{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0yzeb3rJa+JzoBy5nnQGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venchislav/Neural-Networks/blob/main/CustomFittingOnLowerLevel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "z6FaldJ2Rb21"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA LOAD**"
      ],
      "metadata": {
        "id": "e8mZ-SvNSlej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_model():\n",
        "  inputs = keras.Input(shape=(28 * 28,))\n",
        "  features = layers.Dense(512, activation='relu')(inputs)\n",
        "  features = layers.Dropout(0.5)(features)\n",
        "  outputs = layers.Dense(10, activation='softmax')(features)\n",
        "\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model\n",
        "\n",
        "\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60_000, 28 * 28)).astype('float32') / 255.0\n",
        "test_images = test_images.reshape((10_000, 28 * 28)).astype('float32') / 255.0\n",
        "\n",
        "train_images, val_images = images[10_000:], images[:10_000]\n",
        "train_labels, val_labels = labels[10_000:], labels[:10_000]\n"
      ],
      "metadata": {
        "id": "5hXs0IYcLS0d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mnist_model()"
      ],
      "metadata": {
        "id": "EF9CvrFfLZYq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "loss_tracking_metric = keras.metrics.Mean()"
      ],
      "metadata": {
        "id": "Pg0r_4lnRl_c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "  # forward propagation\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  # backpropagation\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  # getting logs (metrics and loss)\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[metric.name] = metric.result()\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs"
      ],
      "metadata": {
        "id": "5WE7JkM8T2D8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NtfbhtM2EeYr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we strictly must reset metrics as it's logical (cmon, think about it, it's logical)\n",
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "A6MdA6tYUyK5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparation\n",
        "#   1. batches split\n",
        "#   2. dataset forming\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_images, train_labels)\n",
        ")\n",
        "\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  reset_metrics()\n",
        "  for inputs_batch, targets_batch in training_dataset:\n",
        "    logs = train_step(inputs_batch, targets_batch)\n",
        "  print(f'results at the end of {epoch} epoch')\n",
        "  for key, value in logs.items():\n",
        "    print(f'...{key}: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXkJs_s9VKXo",
        "outputId": "43eb4828-bb25-489b-acfa-995d5be8a9c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results at the end of 0 epoch\n",
            "...sparse_categorical_accuracy: 0.913919985294342\n",
            "...loss: 0.2888200283050537\n",
            "results at the end of 1 epoch\n",
            "...sparse_categorical_accuracy: 0.9531999826431274\n",
            "...loss: 0.15876807272434235\n",
            "results at the end of 2 epoch\n",
            "...sparse_categorical_accuracy: 0.9638599753379822\n",
            "...loss: 0.1278458535671234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YEAH... We reinvented the bicycle**"
      ],
      "metadata": {
        "id": "zJXzL8RqF7Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's evaluate our model"
      ],
      "metadata": {
        "id": "jyQlv49YGBdN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # here we set training arg to false, as we're not training now\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  logs = {}\n",
        "  for metric in metrics:\n",
        "    metric.update_state(targets, predictions)\n",
        "    logs[metric.name] = metric.result()\n",
        "\n",
        "  loss_tracking_metric.update_state(loss)\n",
        "  logs['loss'] = loss_tracking_metric.result()\n",
        "  return logs, predictions"
      ],
      "metadata": {
        "id": "2u-2xOf8IcNc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "  logs, pred = test_step(inputs_batch, targets_batch)\n",
        "print('Evaluation results: ')\n",
        "for key, value in logs.items():\n",
        "  print(f'...{key}: {value:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFW_8gnbIt60",
        "outputId": "f98bcbee-bb94-40fb-9b8e-557afedc5ade"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: \n",
            "...sparse_categorical_accuracy: 0.9851\n",
            "...loss: 0.0554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sufpj03wQj04",
        "outputId": "454ecf68-9dee-46be-8323-da6e3aeca921"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(str(np.argmax(pred[10])))\n",
        "plt.axis('off')\n",
        "plt.imshow(train_images[10].reshape(28, 28), cmap='gray');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "U44QDyPLQADP",
        "outputId": "32f21466-2ad5-41ae-dcd3-72b492f6c2a9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I CAN TOUCH BONES OF KERAS**"
      ],
      "metadata": {
        "id": "T3M8eceRLL-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the funniest thing here, is @tf.function decorator. This 1 short line speeds up and optimizes our functions"
      ],
      "metadata": {
        "id": "CJJOMuF8JruL"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}